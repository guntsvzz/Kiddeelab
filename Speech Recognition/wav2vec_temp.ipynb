{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC\n",
    "# import torch\n",
    "# import speech_recognition as sr\n",
    "# import io\n",
    "# from pydub import AudioSegment\n",
    "# # load model and processor\n",
    "# processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-large-960h\")\n",
    "# model = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-large-960h\")\n",
    "    \n",
    "# r = sr.Recognizer()\n",
    "\n",
    "# with sr.Microphone(sample_rate = 16000) as source:\n",
    "#     print('Start Speaking now...')\n",
    "#     while True:\n",
    "#         audio = r.listen(source) #pyaudio object\n",
    "#         data = io.BytesIO(audio.get_wav_data()) #list of bytes\n",
    "#         clip = AudioSegment.from_file(data) #numpy array\n",
    "#         x = torch.FloatTensor(clip.get_array_of_samples()) #Tensor\n",
    "        \n",
    "#         inputs = processor(x , samping_rate = 16000 ,return_tensors=\"pt\", padding = 'longest').input_values\n",
    "#         logits = model(inputs).logits\n",
    "#         tokens = torch.argmax(logits, axis = 1) #get the \n",
    "#         text   = processor.batch_decode(tokens) #tokens into a string\n",
    "\n",
    "#         print('You said',str(text).lower())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-large-960h and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Found cached dataset librispeech_asr_dummy (C:/Users/Guntsv/.cache/huggingface/datasets/patrickvonplaten___librispeech_asr_dummy/clean/2.1.0/f2c70a4d03ab4410954901bde48c54b85ca1b7f9bf7d616e7e2a72b5ee6ddbfc)\n",
      "It is strongly recommended to pass the ``sampling_rate`` argument to this function. Failing to do so can result in silent errors that might be hard to debug.\n"
     ]
    }
   ],
   "source": [
    " from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC\n",
    " from datasets import load_dataset\n",
    " import torch\n",
    " \n",
    " # load model and processor\n",
    " processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-large-960h\")\n",
    " model = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-large-960h\")\n",
    "     \n",
    " # load dummy dataset and read soundfiles\n",
    " ds = load_dataset(\"patrickvonplaten/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n",
    " \n",
    " # tokenize\n",
    " input_values = processor(ds[0][\"audio\"][\"array\"], return_tensors=\"pt\", padding=\"longest\").input_values  # Batch size 1\n",
    " \n",
    " # retrieve logits\n",
    " logits = model(input_values).logits\n",
    " \n",
    " # take argmax and decode\n",
    " predicted_ids = torch.argmax(logits, dim=-1)\n",
    " transcription = processor.batch_decode(predicted_ids)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\n",
    "import soundfile as sf\n",
    "import torch\n",
    "from jiwer import wer\n",
    "\n",
    "\n",
    "librispeech_eval = load_dataset(\"librispeech_asr\", \"clean\", split=\"test\")\n",
    "\n",
    "model = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-large-960h\").to(\"cuda\")\n",
    "processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-large-960h\")\n",
    "\n",
    "def map_to_pred(batch):\n",
    "    input_values = processor(batch[\"audio\"][\"array\"], return_tensors=\"pt\", padding=\"longest\").input_values\n",
    "    with torch.no_grad():\n",
    "        logits = model(input_values.to(\"cuda\")).logits\n",
    "\n",
    "    predicted_ids = torch.argmax(logits, dim=-1)\n",
    "    transcription = processor.batch_decode(predicted_ids)\n",
    "    batch[\"transcription\"] = transcription\n",
    "    return batch\n",
    "\n",
    "result = librispeech_eval.map(map_to_pred, batched=True, batch_size=1, remove_columns=[\"speech\"])\n",
    "\n",
    "print(\"WER:\", wer(result[\"text\"], result[\"transcription\"]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c81d839d3c4227cd770621df97fe8191838af02e7eef185a922d8250cb33d344"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
