{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-large-960h and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Speaking now...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python3.10.4\\lib\\site-packages\\pydub\\utils.py:198: RuntimeWarning: Couldn't find ffprobe or avprobe - defaulting to ffprobe, but may not work\n",
      "  warn(\"Couldn't find ffprobe or avprobe - defaulting to ffprobe, but may not work\", RuntimeWarning)\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 2] The system cannot find the file specified",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Guntsv\\Documents\\GitHub\\Kiddeelab\\Speech Recognition\\wav2vec_temp.ipynb Cell 1\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Guntsv/Documents/GitHub/Kiddeelab/Speech%20Recognition/wav2vec_temp.ipynb#W0sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m audio \u001b[39m=\u001b[39m r\u001b[39m.\u001b[39mlisten(source) \u001b[39m#pyaudio object\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Guntsv/Documents/GitHub/Kiddeelab/Speech%20Recognition/wav2vec_temp.ipynb#W0sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m data \u001b[39m=\u001b[39m io\u001b[39m.\u001b[39mBytesIO(audio\u001b[39m.\u001b[39mget_wav_data()) \u001b[39m#list of bytes\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Guntsv/Documents/GitHub/Kiddeelab/Speech%20Recognition/wav2vec_temp.ipynb#W0sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m clip \u001b[39m=\u001b[39m AudioSegment\u001b[39m.\u001b[39;49mfrom_file(data) \u001b[39m#numpy array\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Guntsv/Documents/GitHub/Kiddeelab/Speech%20Recognition/wav2vec_temp.ipynb#W0sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mFloatTensor(clip\u001b[39m.\u001b[39mget_array_of_samples()) \u001b[39m#Tensor\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Guntsv/Documents/GitHub/Kiddeelab/Speech%20Recognition/wav2vec_temp.ipynb#W0sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m inputs \u001b[39m=\u001b[39m processor(x , samping_rate \u001b[39m=\u001b[39m \u001b[39m16000\u001b[39m ,return_tensors\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m, padding \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mlongest\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39minput_values\n",
      "File \u001b[1;32mc:\\Python3.10.4\\lib\\site-packages\\pydub\\audio_segment.py:728\u001b[0m, in \u001b[0;36mAudioSegment.from_file\u001b[1;34m(cls, file, format, codec, parameters, start_second, duration, **kwargs)\u001b[0m\n\u001b[0;32m    726\u001b[0m     info \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    727\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 728\u001b[0m     info \u001b[39m=\u001b[39m mediainfo_json(orig_file, read_ahead_limit\u001b[39m=\u001b[39;49mread_ahead_limit)\n\u001b[0;32m    729\u001b[0m \u001b[39mif\u001b[39;00m info:\n\u001b[0;32m    730\u001b[0m     audio_streams \u001b[39m=\u001b[39m [x \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m info[\u001b[39m'\u001b[39m\u001b[39mstreams\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m    731\u001b[0m                      \u001b[39mif\u001b[39;00m x[\u001b[39m'\u001b[39m\u001b[39mcodec_type\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39maudio\u001b[39m\u001b[39m'\u001b[39m]\n",
      "File \u001b[1;32mc:\\Python3.10.4\\lib\\site-packages\\pydub\\utils.py:274\u001b[0m, in \u001b[0;36mmediainfo_json\u001b[1;34m(filepath, read_ahead_limit)\u001b[0m\n\u001b[0;32m    271\u001b[0m         file\u001b[39m.\u001b[39mclose()\n\u001b[0;32m    273\u001b[0m command \u001b[39m=\u001b[39m [prober, \u001b[39m'\u001b[39m\u001b[39m-of\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mjson\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m+\u001b[39m command_args\n\u001b[1;32m--> 274\u001b[0m res \u001b[39m=\u001b[39m Popen(command, stdin\u001b[39m=\u001b[39;49mstdin_parameter, stdout\u001b[39m=\u001b[39;49mPIPE, stderr\u001b[39m=\u001b[39;49mPIPE)\n\u001b[0;32m    275\u001b[0m output, stderr \u001b[39m=\u001b[39m res\u001b[39m.\u001b[39mcommunicate(\u001b[39minput\u001b[39m\u001b[39m=\u001b[39mstdin_data)\n\u001b[0;32m    276\u001b[0m output \u001b[39m=\u001b[39m output\u001b[39m.\u001b[39mdecode(\u001b[39m\"\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mignore\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Python3.10.4\\lib\\subprocess.py:966\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[1;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, user, group, extra_groups, encoding, errors, text, umask, pipesize)\u001b[0m\n\u001b[0;32m    962\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtext_mode:\n\u001b[0;32m    963\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstderr \u001b[39m=\u001b[39m io\u001b[39m.\u001b[39mTextIOWrapper(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstderr,\n\u001b[0;32m    964\u001b[0m                     encoding\u001b[39m=\u001b[39mencoding, errors\u001b[39m=\u001b[39merrors)\n\u001b[1;32m--> 966\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_execute_child(args, executable, preexec_fn, close_fds,\n\u001b[0;32m    967\u001b[0m                         pass_fds, cwd, env,\n\u001b[0;32m    968\u001b[0m                         startupinfo, creationflags, shell,\n\u001b[0;32m    969\u001b[0m                         p2cread, p2cwrite,\n\u001b[0;32m    970\u001b[0m                         c2pread, c2pwrite,\n\u001b[0;32m    971\u001b[0m                         errread, errwrite,\n\u001b[0;32m    972\u001b[0m                         restore_signals,\n\u001b[0;32m    973\u001b[0m                         gid, gids, uid, umask,\n\u001b[0;32m    974\u001b[0m                         start_new_session)\n\u001b[0;32m    975\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[0;32m    976\u001b[0m     \u001b[39m# Cleanup if the child failed starting.\u001b[39;00m\n\u001b[0;32m    977\u001b[0m     \u001b[39mfor\u001b[39;00m f \u001b[39min\u001b[39;00m \u001b[39mfilter\u001b[39m(\u001b[39mNone\u001b[39;00m, (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstdin, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstdout, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstderr)):\n",
      "File \u001b[1;32mc:\\Python3.10.4\\lib\\subprocess.py:1435\u001b[0m, in \u001b[0;36mPopen._execute_child\u001b[1;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, unused_restore_signals, unused_gid, unused_gids, unused_uid, unused_umask, unused_start_new_session)\u001b[0m\n\u001b[0;32m   1433\u001b[0m \u001b[39m# Start the process\u001b[39;00m\n\u001b[0;32m   1434\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1435\u001b[0m     hp, ht, pid, tid \u001b[39m=\u001b[39m _winapi\u001b[39m.\u001b[39;49mCreateProcess(executable, args,\n\u001b[0;32m   1436\u001b[0m                              \u001b[39m# no special security\u001b[39;49;00m\n\u001b[0;32m   1437\u001b[0m                              \u001b[39mNone\u001b[39;49;00m, \u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m   1438\u001b[0m                              \u001b[39mint\u001b[39;49m(\u001b[39mnot\u001b[39;49;00m close_fds),\n\u001b[0;32m   1439\u001b[0m                              creationflags,\n\u001b[0;32m   1440\u001b[0m                              env,\n\u001b[0;32m   1441\u001b[0m                              cwd,\n\u001b[0;32m   1442\u001b[0m                              startupinfo)\n\u001b[0;32m   1443\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m   1444\u001b[0m     \u001b[39m# Child is launched. Close the parent's copy of those pipe\u001b[39;00m\n\u001b[0;32m   1445\u001b[0m     \u001b[39m# handles that only the child should have open.  You need\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1448\u001b[0m     \u001b[39m# pipe will not close when the child process exits and the\u001b[39;00m\n\u001b[0;32m   1449\u001b[0m     \u001b[39m# ReadFile will hang.\u001b[39;00m\n\u001b[0;32m   1450\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_close_pipe_fds(p2cread, p2cwrite,\n\u001b[0;32m   1451\u001b[0m                          c2pread, c2pwrite,\n\u001b[0;32m   1452\u001b[0m                          errread, errwrite)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 2] The system cannot find the file specified"
     ]
    }
   ],
   "source": [
    "from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC\n",
    "import torch\n",
    "import speech_recognition as sr\n",
    "import io\n",
    "from pydub import AudioSegment\n",
    "# load model and processor\n",
    "processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-large-960h\")\n",
    "model = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-large-960h\")\n",
    "    \n",
    "r = sr.Recognizer()\n",
    "\n",
    "with sr.Microphone(sample_rate = 16000) as source:\n",
    "    print('Start Speaking now...')\n",
    "    while True:\n",
    "        audio = r.listen(source) #pyaudio object\n",
    "        data = io.BytesIO(audio.get_wav_data()) #list of bytes\n",
    "        clip = AudioSegment.from_file(data) #numpy array\n",
    "        x = torch.FloatTensor(clip.get_array_of_samples()) #Tensor\n",
    "        \n",
    "        inputs = processor(x , samping_rate = 16000 ,return_tensors=\"pt\", padding = 'longest').input_values\n",
    "        logits = model(inputs).logits\n",
    "        tokens = torch.argmax(logits, axis = 1) #get the \n",
    "        text   = processor.batch_decode(tokens) #tokens into a string\n",
    "\n",
    "        print('You said',str(text).lower())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers\n",
    "# !pip install datasets\n",
    "import soundfile as sf\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\n",
    "\n",
    "# load pretrained model\n",
    "processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "model = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "\n",
    "\n",
    "librispeech_samples_ds = load_dataset(\"patrickvonplaten/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n",
    "\n",
    "# load audio\n",
    "audio_input, sample_rate = sf.read(librispeech_samples_ds[0][\"file\"])\n",
    "\n",
    "# pad input values and return pt tensor\n",
    "input_values = processor(audio_input, sampling_rate=sample_rate, return_tensors=\"pt\").input_values\n",
    "\n",
    "# INFERENCE\n",
    "\n",
    "# retrieve logits & take argmax\n",
    "logits = model(input_values).logits\n",
    "predicted_ids = torch.argmax(logits, dim=-1)\n",
    "\n",
    "# transcribe\n",
    "transcription = processor.decode(predicted_ids[0])\n",
    "\n",
    "# FINE-TUNE\n",
    "\n",
    "target_transcription = \"A MAN SAID TO THE UNIVERSE I EXIST\"\n",
    "\n",
    "# encode labels\n",
    "with processor.as_target_processor():\n",
    "  labels = processor(target_transcription, return_tensors=\"pt\").input_ids\n",
    "\n",
    "# compute loss by passing labels\n",
    "loss = model(input_values, labels=labels).loss\n",
    "loss.backward()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c81d839d3c4227cd770621df97fe8191838af02e7eef185a922d8250cb33d344"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
